{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Matthew.Hanauer\\\\Anaconda3\\\\lib\\\\site-packages\\\\pyspark']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import numpy\n",
    "import pyspark\n",
    "import findspark\n",
    "import pandas as pd\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.readwriter import DataFrameReader\n",
    "from pyspark.sql.streaming import DataStreamReader\n",
    "from pyspark.sql.utils import install_exception_handler\n",
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "sys.path\n",
    "numpy.__path__\n",
    "pyspark.__path__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars xgboost4j-spark-0.72.jar,xgboost4j-0.72.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('C:\\\\Users\\\\Matthew.Hanauer\\\\Anaconda3\\\\lib\\\\site-packages\\\\pyspark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4048\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opens up the connection to Spark\n",
    "#sc =SparkContext()\n",
    "## This allows you to look at the data in a sql form which helps make sure the data is loaded in correctly\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Quarter|\n",
      "+-------+\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      1|\n",
      "|      2|\n",
      "|      1|\n",
      "|      3|\n",
      "|      1|\n",
      "|      3|\n",
      "|      1|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|           Quarter|\n",
      "+-------+------------------+\n",
      "|  count|              3603|\n",
      "|   mean| 2.877879544823758|\n",
      "| stddev|0.8516025002060255|\n",
      "|    min|                 1|\n",
      "|    max|                 4|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, Quarter: int, Agegroup: int, OverallHealth: int, CapableManagingHealthCareNeeds: int, HandlingDailyLife: int, ControlLife: int, DealWithCrisis: int, GetsAlongWithFamily: int, SocialSituations: int, FunctioningHousing: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pyspark_test_dat\n",
    "#Pyspark data test\n",
    "keras_dat = pd.read_csv(\"keras_test.csv\", delimiter = \",\")\n",
    "keras_dat = keras_dat.iloc[:,0:10]\n",
    "keras_dat = keras_dat.rename(columns = {\"Quarter.x\": \"Quarter\", \"Agegroup.x\": \"Agegroup\", \"OverallHealth.x\": \"OverallHealth\", \"CapableManagingHealthCareNeeds.x\": \"CapableManagingHealthCareNeeds\", \"HandlingDailyLife.x\": \"HandlingDailyLife\", \"ControlLife.x\": \"ControlLife\", \"DealWithCrisis.x\": \"DealWithCrisis\", \"GetsAlongWithFamily.x\": \"GetsAlongWithFamily\", \"SocialSituations.x\": \"SocialSituations\", \"FunctioningHousing.x\": \"FunctioningHousing\"})\n",
    "pyspark_test_dat = keras_dat\n",
    "pyspark_test_dat.to_csv(\"pyspark_test_dat.csv\")\n",
    "test =  sqlContext.read.csv(\"pyspark_test_dat.csv\", inferSchema = True, header = True)\n",
    "#test.show()\n",
    "#test.printSchema()\n",
    "#len(test.columns)\n",
    "#test.count()\n",
    "test.select('Quarter').show()\n",
    "test.describe('Quarter').show()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.010387592342173162"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import six\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "test.stat.corr(\"Quarter\", \"Agegroup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|            features|OverallHealth|\n",
      "+--------------------+-------------+\n",
      "|[2.0,8.0,3.0,2.0,...|            4|\n",
      "|[1.0,6.0,1.0,4.0,...|            1|\n",
      "|[2.0,8.0,1.0,4.0,...|            4|\n",
      "+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = [\"Quarter\", \"Agegroup\", \"CapableManagingHealthCareNeeds\", \"HandlingDailyLife\", \"ControlLife\", \"DealWithCrisis\", \"GetsAlongWithFamily\", \"SocialSituations\", \"FunctioningHousing\"], outputCol = 'features')\n",
    "vhouse_df = vectorAssembler.transform(test)\n",
    "vhouse_df = vhouse_df.select(['features', 'OverallHealth'])\n",
    "vhouse_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[OverallHealth: int]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vhouse_df.select(['features'])\n",
    "vhouse_df.select(['OverallHealth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vhouse_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.04202702080411247,0.0,-0.05473149288184722,-0.08696620341232235,-0.007142282891801386,0.0,-0.01900258173289042,-0.0021236662543530405]\n",
      "Intercept: 3.524638834685568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.ml.regression.LinearRegressionTrainingSummary at 0x18122433508>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='OverallHealth', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))\n",
    "lr_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+--------------------+\n",
      "|        prediction|OverallHealth|            features|\n",
      "+------------------+-------------+--------------------+\n",
      "| 3.535256504215492|            2|[1.0,1.0,4.0,4.0,...|\n",
      "| 3.189858860864739|            3|[1.0,2.0,1.0,2.0,...|\n",
      "| 1.455552888180584|            3|[1.0,2.0,1.0,5.0,...|\n",
      "| 3.012034752097057|            1|[1.0,2.0,1.0,5.0,...|\n",
      "|1.5798403505431151|            3|[1.0,2.0,2.0,4.0,...|\n",
      "+------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#gbm go here for hyper parameters to tune: https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/ml/regression/GBTRegressor.html\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'OverallHealth', maxIter=10, maxDepth = 10)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_predictions.select('prediction', 'OverallHealth', 'features').show(5)\n",
    "pred_gbt =  gbt_predictions.select('prediction', 'OverallHealth', 'features')\n",
    "pred_gbt.toPandas().to_csv(\"pred_gbt.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
